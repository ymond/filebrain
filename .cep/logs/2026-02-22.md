## Session: 2026-02-22

### Summary
Built the entire core system from the storage layer through to the CLI in a single
session. Implemented the SQLite metadata store, Qdrant vector store (switched from
ChromaDB due to Python 3.14 incompatibility), text chunker with sentence-aware
splitting, embedding generator via ollama, the processing pipeline that wires
everything together, the filesystem watcher (batch scan + live inotify monitoring),
the query engine with RAG-powered answers and source citations, and a CLI with scan,
watch, query, and status commands. 130 tests pass, covering every component. The
system is fully functional — you can scan a directory, watch it for changes, and
ask questions about your files with cited answers from qwen2.5:7b.

### Decisions Made
- **Decision:** Switch from ChromaDB to Qdrant for vector storage (ADR-002)
  - **Alternatives considered:** ChromaDB (original plan), FAISS
  - **Why this choice:** ChromaDB 1.5.1 crashes on Python 3.14 due to pydantic v1
    dependency. Qdrant's local embedded mode works perfectly, offers better performance,
    and supports persistent local storage without a server process.
  - **Reversibility:** Medium — VectorStore abstraction makes swapping possible but
    existing embeddings would need to be regenerated

- **Decision:** Sentence-aware text chunking (ADR-003)
  - **Alternatives considered:** Fixed-size character chunks, recursive/semantic splitting
  - **Why this choice:** Good balance of semantic coherence and simplicity. 1000-char
    chunks with 200-char overlap work well with nomic-embed-text.
  - **Reversibility:** Easy — change parameters and re-process

- **Decision:** Structured RAG prompt with citation rules (ADR-004)
  - **Alternatives considered:** Simple context+question, JSON-structured output
  - **Why this choice:** Clear instructions for source citation, hallucination guard,
    reliable with qwen2.5:7b
  - **Reversibility:** Easy — just change the prompt string

- **Decision:** Enable SQLite `check_same_thread=False` for cross-thread access
  - **Alternatives considered:** Creating separate connections per thread, using a queue
  - **Why this choice:** Simplest fix for watchdog's observer thread. WAL mode makes
    concurrent reads safe; our writes are serialized by the event loop.
  - **Reversibility:** Easy

- **Decision:** Use urllib from stdlib for ollama API calls (no httpx/requests)
  - **Alternatives considered:** httpx, requests library
  - **Why this choice:** Zero dependency — ollama's API is simple REST, no auth needed,
    no complex HTTP features required
  - **Reversibility:** Easy

### Mikado Tree Progress
- Active path: goal (all core nodes complete, only audio/video stretch goal remains)
- [x] Storage layer (MHC 11)
  - [x] SQLite metadata store (MHC 10) — 23 tests, commit bb73260
  - [x] Vector store / Qdrant (MHC 10) — 12 tests, commit c7776f3
- [x] Embedding pipeline (MHC 10)
  - [x] Text chunking (MHC 10) — 13 tests, commit b3c1704
  - [x] Embedding generation (MHC 9) — 7 tests, commit c20d7e7
  - [x] Processing pipeline (MHC 11) — 11 tests, commit 0baaf4d
- [x] File watcher daemon (MHC 10) — 6 tests, commit 071af82
- [x] Query interface (MHC 11)
  - [x] Semantic search (MHC 9) — 3 tests
  - [x] LLM-powered Q&A (MHC 11) — 3 tests, commit 2ef4b3d
  - [x] CLI interface (MHC 9) — 4 tests, commit a9c1a76
- [ ] Audio/video transcription (MHC 10) — stretch goal, not started

### TDD Cycle Log
1. **MetadataStore:** 23 tests covering init, upsert/get, change detection, status
   queries, status transitions, deletion, and counting. All failed (ModuleNotFoundError).
   Implemented `metadata.py` with SQLite schema, FileRecord dataclass, FileStatus enum.
   All 23 passed first try. Later fixed cross-thread safety for watchdog integration.

2. **VectorStore:** 12 tests covering init (memory + persistent), add/search, ranked
   results, limit, deletion by source path, counting. First attempted ChromaDB —
   crashed on Python 3.14. Switched to qdrant-client which worked perfectly. All 12
   passed first try.

3. **TextChunker:** 13 tests covering short text passthrough, empty/whitespace input,
   multi-chunk splitting, size limits, no content loss, overlap verification, sentence
   boundaries, fallback char splitting, and default values. All 13 passed first try.

4. **EmbeddingGenerator:** 7 tests (integration with ollama) covering embedding
   generation, dimensionality (768), different texts produce different vectors,
   similar texts have high cosine similarity, batch mode, and custom model name.
   All 7 passed first try.

5. **ProcessingPipeline:** 11 tests with FakeEmbeddingGenerator. End-to-end file
   processing, Python files, content hash, multi-chunk, skip unchanged, reprocess
   changed, unsupported file failure, missing file failure, failure isolation,
   directory processing, and stats reporting. 10 passed first try; the directory
   test failed because SQLite .db files in tmp_path got processed. Fixed by separating
   db_dir from scan_dir in the test fixture. All 11 passed.

6. **FileWatcher:** 6 tests — 3 batch scan (all files, skip processed, pick up changed),
   3 live events (detect new, modified, deleted). All 3 batch tests passed. All 3
   live tests failed: SQLite thread safety error (watchdog observer runs in a separate
   thread). Fixed with `check_same_thread=False` on the SQLite connection. All 6 passed.

7. **QueryEngine:** 6 tests — 3 SearchOnlyEngine unit tests (returns results, empty
   store, limit), 3 QueryEngine integration tests with real ollama (answer with
   citations, real file paths, structured result). All 6 passed first try.

8. **CLI:** 4 tests — scan integration, status output, help text verification,
   status with no db. 3 passed first try. The status-no-db test failed due to
   argparse argument ordering (--db-dir must precede subcommand). Fixed test. All 4 passed.

### What I Learned (for Raymond)

**ChromaDB doesn't work with Python 3.14.** The latest version (1.5.1) depends on
pydantic v1 internally, which crashes when trying to introspect type annotations
under Python 3.14's changed internals. This is a known gap — the pydantic v1 shim
in pydantic v2 doesn't support Python 3.14. Qdrant's local embedded mode turned out
to be a better choice anyway: it bundles a Rust engine in the Python wheel, so you
get HNSW indexing and high performance without running a server.

**SQLite's thread safety model.** By default, Python's `sqlite3` module enforces that
a connection object can only be used from the thread that created it. This is a safety
check (`check_same_thread=True` by default), not a fundamental SQLite limitation.
With WAL (Write-Ahead Logging) journal mode, SQLite supports concurrent readers and
a single writer safely. Since our watcher's event handler (running in the watchdog
observer thread) needs to write to the same database the main thread reads from,
we enable `check_same_thread=False` and rely on WAL for correctness. For our use
case (one writer at a time, serialized by event delivery), this is safe.

**ollama's batch embedding API.** The `/api/embed` endpoint accepts either a string
or a list of strings as the `input` field. When you pass a list, it returns multiple
embeddings in a single roundtrip. This is much faster than calling embed() in a loop
for each chunk. The response has an `embeddings` key (plural) containing a list of
vectors.

**Qdrant's local embedded mode.** `QdrantClient(path="./data")` gives you a fully
persistent vector database backed by local files — no server process needed. For
testing, `QdrantClient(":memory:")` works like SQLite's in-memory mode. The Python
client bundles the Qdrant engine as a Rust extension, so there's no separate install.

**argparse subcommand argument ordering.** When using argparse subparsers, global
arguments (like `--db-dir`) must appear *before* the subcommand name. So
`filebrain --db-dir /tmp status` works, but `filebrain status --db-dir /tmp` doesn't.
This is a common argparse gotcha.

### Open Questions
- The audio/video transcription extractor (stretch goal) is the only remaining
  Mikado tree node. Should we proceed with it in the next session, or is there
  something else Raymond wants to prioritize?
- The system is fully functional but hasn't been tested on a real directory of
  Raymond's files. A real-world test would reveal performance characteristics
  and edge cases (very large files, unusual encodings, deeply nested directories).

### Possibilities
- **Incremental re-scanning with file system timestamps:** Instead of hashing every
  file to detect changes, check mtime first and only hash if mtime differs. Much
  faster for large directory trees.
- **Parallel processing:** Use a thread pool or process pool to extract/embed multiple
  files concurrently. The RTX 5070 can handle multiple embedding requests.
- **TUI with rich:** The `rich` library (already a transitive dependency via qdrant)
  could power a nice terminal UI showing scan progress, search results with highlighted
  matches, and status dashboards.
- **Configurable ignore patterns:** Like .gitignore, let users specify directories
  and file patterns to skip (node_modules, .git, __pycache__, etc.).
- **Deduplication:** Files with identical content hashes could share embeddings
  instead of storing duplicates.
- **Image extraction:** OCR for images (.png, .jpg) using tesseract, and image
  captioning using a local multimodal model.
